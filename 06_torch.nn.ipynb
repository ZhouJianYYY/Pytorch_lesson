{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *class torch.nn.Parameters()*    \n",
    "        data    \n",
    "        requires_grads(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *class torch.nn.Module*   \n",
    "        add_module(name, module)     \n",
    "        children()   \n",
    "        cpu()     \n",
    "        cuda()   \n",
    "        train()   \n",
    "        eval()   \n",
    "        load_state_dict()   \n",
    "        modules()   \n",
    "        parameters()  \n",
    "        state_dict()   \n",
    "        zero_grads()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *class torch.nn.Sequential()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *class torch.nn.ModuleList()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for _ in range(10)])\n",
    "    def forward(self,x):\n",
    "        for i,l in enumerate(self.linears):\n",
    "            x = self.linears[i//2] + l(x)\n",
    "        return x\n",
    "mynet = MyModule()\n",
    "mynet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积 Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.Conv1d(in_channels,out_channels,kernel_size,  \n",
    "           stride=1,paddding=0,dilation=1,groups=1,bias=True)**\n",
    "$$ out(N_i, C_{out_j})=bias(C {out_j})+\\sum^{C{in}-1}{k=0}weight(C{out_j},k)\\bigotimes input(N_i,k) $$\n",
    "* 参数说明：   \n",
    "    in_channels(int) – 输入信号的通道  \n",
    "    out_channels(int) – 卷积产生的通道   \n",
    "    kerner_size(int or tuple) - 卷积核的尺寸   \n",
    "    stride(int or tuple, optional) - 卷积步长   \n",
    "    padding (int or tuple, optional)- 输入的每一条边补充0的层数   \n",
    "    dilation(int or tuple, `optional``) – 卷积核元素之间的间距   \n",
    "    groups(int, optional) – 从输入通道到输出通道的阻塞连接数   \n",
    "    bias(bool, optional) - 如果bias=True，添加偏置   \n",
    "卷积核大小为 （kernel_size,embedding_size)\n",
    "   \n",
    "输入: (N,C_in,L_in)    \n",
    "输出: (N,C_out,L_out)   \n",
    "输入输出的计算方式：   \n",
    "$$L_{out}=floor((L_{in}+2padding-dilation(kernel\\_size-1)-1)/stride+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 48])\n"
     ]
    }
   ],
   "source": [
    "m=nn.Conv1d(16,33,3,stride=1)\n",
    "input = torch.randn(20,16,50)\n",
    "output = m(input)\n",
    "print(output.size())\n",
    "\n",
    "# 48 = (50-3)/1 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 16, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.Conv2d(in_channels,out_channels,kernel_size,   \n",
    "    stride=1,padding=0,dilation=1,groups=1,bias=True)**\n",
    "* 参数说明同Conv1d\n",
    "* shape:   \n",
    "input: (N,C_in,H_in,W_in)   \n",
    "output: (N,C_out,H_out,W_out)   \n",
    "\n",
    "$$H_{out}=floor((H_{in}+2padding[0]-dilation[0](kernerl\\_size[0]-1)-1)/stride[0]+1)$$   \n",
    "\n",
    "$$W_{out}=floor((W_{in}+2padding[1]-dilation[1](kernerl\\_size[1]-1)-1)/stride[1]+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "m =nn.Conv2d(16,32,3,stride=1,padding=1)\n",
    "input = torch.randn(20,16,32,32)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.Conv3d(in_channels,out_channels,kernel_size, stride=1,padding=0,dilation=1,groups=1,bias=True)**\n",
    "* 参数说明同Conv2d  \n",
    "* shape：   \n",
    "input: (N,C_in,D_in,H_in,W_in)   \n",
    "output: (N,C_out,D_out,H_out,W_out) \n",
    "\n",
    "$$D_{out}=floor((D_{in}+2padding[0]-dilation[0](kernerl\\_size[0]-1)-1)/stride[0]+1)$$  \n",
    "\n",
    "$$H_{out}=floor((H_{in}+2padding[1]-dilation[2](kernerl\\_size[1]-1)-1)/stride[1]+1)$$\n",
    "\n",
    "$$W_{out}=floor((W_{in}+2padding[2]-dilation[2](kernerl\\_size[2]-1)-1)/stride[2]+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 30, 30, 26])\n"
     ]
    }
   ],
   "source": [
    "m =nn.Conv3d(16,32,3,stride=1,padding=0)\n",
    "input = torch.randn(20,16,32,32,28)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvTranspose1d   \n",
    "1维的解卷积操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.ConvTranspose1d(in_channels,out_channels,kernel_size,\n",
    "stride=1,padding=0,ouput_padding=0,groups=1,bias=True)**\n",
    " \n",
    "* shape:   \n",
    "input: (N,C_in,L_in)   \n",
    "output: (N,C_out,L_out)  \n",
    "$$L_{out}=(L_{in}-1)stride-2padding+kernel\\_size+output\\_padding$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 20])\n"
     ]
    }
   ],
   "source": [
    "dconv = nn.ConvTranspose1d(20,16,3)\n",
    "input =torch.randn(32,20,18)\n",
    "output=dconv(input)\n",
    "print(output.size())\n",
    "# 20 = （18-1）*1 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvTranspose2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size, stride=1,padding=0,ouput_padding=0,groups=1,bias=True)**\n",
    "* 参数说明同ConvTranspose1d\n",
    "* shape:    \n",
    "input: (N,C_in,H_in，W_in)      \n",
    "output: (N,C_out,H_out,W_out)   \n",
    "\n",
    "$$H_{out}=(H_{in}-1)stride[0]-2padding[0]+kernel\\_size[0]+output\\_padding[0]$$\n",
    "\n",
    "$$W_{out}=(W_{in}-1)stride[1]-2padding[1]+kernel\\_size[1]+output\\_padding[1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 101, 201])\n",
      "torch.Size([20, 33, 93, 100])\n"
     ]
    }
   ],
   "source": [
    "m1 = nn.ConvTranspose2d(16,33,3,stride=2)\n",
    "m2 = nn.ConvTranspose2d(16,33,(3,5),stride=(2,1),padding=(4,2))\n",
    "input = torch.randn(20,16,50,100)\n",
    "out1 = m1(input)\n",
    "out2 = m2(input)\n",
    "print(out1.size())\n",
    "print(out2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvTranspose3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size, stride=1,padding=0,ouput_padding=0,groups=1,bias=True)**\n",
    "* shape:   \n",
    "input: (N,C_in,H_in，W_in)   \n",
    "output: (N,C_out,H_out,W_out)   \n",
    "\n",
    "$$D_{out}=(D_{in}-1)stride[0]-2padding[0]+kernel\\_size[0]+output\\_padding[0]$$\n",
    "\n",
    "$$H_{out}=(H_{in}-1)stride[1]-2padding[1]+kernel\\_size[1]+output\\_padding[0]$$\n",
    "\n",
    "$$W_{out}=(W_{in}-1)stride[2]-2padding[2]+kernel\\_size[2]+output\\_padding[2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPool1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,\n",
    "                          dilation=1,return_indices=False,ceil_mode=False)**\n",
    "* 参数说明：   \n",
    "kernel_size(int or tuple) - max pooling的窗口大小   \n",
    "stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size   \n",
    "padding(int or tuple, optional) - 输入的每一条边补充0的层数   \n",
    "dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数    \n",
    "return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助   \n",
    "ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作  \n",
    "* shape:   \n",
    "输入: (N,C_in,L_in)   \n",
    "输出: (N,C_out,L_out)   \n",
    "\n",
    "$$L_{out}=floor((L_{in} + 2padding - dilation(kernel\\_size - 1) - 1)/stride + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxPool2d(kernel_size,stride=None,    padding=0,dilation=1,return_indices=False,ceil_mode=False)**\n",
    "* 参数说明同Maxpool1d \n",
    "* shape:   \n",
    "输入: (N,C,H_in,W_in)   \n",
    "输出: (N,C,H_out,W_out)   \n",
    "\n",
    "$$H_{out}=floor((H_{in} + 2padding[0] - dilation[0](kernel\\_size[0] - 1) - 1)/stride[0] + 1$$\n",
    "\n",
    "$$W_{out}=floor((W_{in} + 2padding[1] - dilation[1](kernel\\_size[1] - 1) - 1)/stride[1] + 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1, return_indices=False,ceil_mode=False)**   \n",
    "* 参数说明同Maxpool1d \n",
    "* shape:   \n",
    "输入: (N,C,D_in,H_in,W_in)   \n",
    "输出: (N,C,D_out,H_out,W_out)   \n",
    "\n",
    "$$D_{out}=floor((D_{in} + 2padding[0] - dilation[0](kernel\\_size[0] - 1) - 1)/stride[0] + 1)$$\n",
    "\n",
    "$$H_{out}=floor((H_{in} + 2padding[1] - dilation[1](kernel\\_size[0] - 1) - 1)/stride[1] + 1)$$\n",
    "\n",
    "$$W_{out}=floor((W_{in} + 2padding[2] - dilation[2](kernel\\_size[2] - 1) - 1)/stride[2] + 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxUnPool1d   \n",
    "Maxpool1d的逆过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)**\n",
    "* 输入：   \n",
    "input:需要转换的tensor    \n",
    "indices：Maxpool1d的索引号    \n",
    "output_size:一个指定输出大小的torch.Size  \n",
    "* shape :   \n",
    "input: (N,C,H_in)  \n",
    "output:(N,C,H_out)  \n",
    "\n",
    "$$H_{out}=(H_{in}-1)stride[0]-2padding[0]+kernel\\_size[0]$$\n",
    "也可以使用output_size指定输出的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.,  0.,  0.,  4.,  0.,  6.,  0.,  8.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool1d(2,stride=2,return_indices=True)\n",
    "unpool = nn.MaxUnpool1d(2,stride=2)\n",
    "input = torch.Tensor([[[10,2,3,4,5,6,7,8]]])\n",
    "output,indices = pool(input)\n",
    "unpool(output,indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxUnpool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)**\n",
    "* 输入：   \n",
    "input:需要转换的tensor    \n",
    "indices：Maxpool1d的索引号    \n",
    "output_size:一个指定输出大小的torch.Size  \n",
    "* shape :   \n",
    "input: (N,C,H_in,W_in)   \n",
    "output:(N,C,H_out,W_out)    \n",
    "\n",
    "$$H_{out}=(H_{in}-1)stride[0]-2padding[0]+kernel\\_size[0]$$\n",
    "\n",
    "$$W_{out}=(W_{in}-1)stride[1]-2padding[1]+kernel\\_size[1]$$\n",
    "\n",
    "也可以使用output_size指定输出的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxUnpool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)**\n",
    "* 输入：   \n",
    "input:需要转换的tensor    \n",
    "indices：Maxpool1d的索引号    \n",
    "output_size:一个指定输出大小的torch.Size  \n",
    "* shape :   \n",
    "input: (N,C,D_in,H_in,W_in)\n",
    "output:(N,C,D_out,H_out,W_out)\n",
    "    \n",
    "$$ D_{out}=(D_{in}-1)stride[0]-2padding[0]+kernel\\_size[0]$$\n",
    "\n",
    "$$H_{out}=(H_{in}-1)stride[1]-2padding[0]+kernel\\_size[1]  $$\n",
    "\n",
    "$$W_{out}=(W_{in}-1)stride[2]-2padding[2]+kernel\\_size[2]  $$\n",
    "\n",
    "也可以使用output_size指定输出的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AvgPool1d\n",
    "## AvgPool2d\n",
    "## AvgPool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.AvgPool2d(kernel_size, stride=None, \n",
    "                           padding=0, ceil_mode=False, count_include_pad=True)**\n",
    "* 参数说明：                           \n",
    "count_include_pad - 如果等于True，计算平均池化时，将包括padding填充的0\n",
    "* shape:    \n",
    "input: (N,C,H_in,W_in)  \n",
    "output: (N,C,H_out,W_out)   \n",
    "$$H_{out}=floor((H_{in}+2padding[0]-kernel_size[0])/stride[0]+1) $$\n",
    "\n",
    " $$W_{out}=floor((W_{in}+2padding[1]-kernel_size[1])/stride[1]+1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非线性层 （激活函数）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ReLU\n",
    "## nn.ReLU6  \n",
    "* ${ReLU6}(x) = min(max(0,x), 6)$ \n",
    "\n",
    "## nn.ELU  \n",
    "* $f(x) = max(0,x) + min(0, alpha * (e^x - 1))$ alpha=1.0\n",
    "\n",
    "## nn.PReLU\n",
    "nn.PReLU(num_parameters=1, init=0.25)\n",
    "\n",
    "num_parameters：需要学习的a的个数，默认等于1   \n",
    "init：a的初始值，默认等于0.25\n",
    "\n",
    "* $PReLU(x) = max(0,x) + a * min(0,x)$\n",
    "\n",
    "## nn.LeakyReLU\n",
    "nn.LeakyReLU(negative_slope=0.01, inplace=False) \n",
    "\n",
    "## nn.Threshold\n",
    "nn.Threshold(threshold, value, inplace=False) \n",
    "\n",
    "$y=x,if x>=threshold; \\ y=value,if x<threshold$\n",
    "\n",
    "## nn.Hardtanh\n",
    "nn.Hardtanh(min_value=-1, max_value=1, inplace=False)\n",
    "\n",
    "$f(x)=+1,if\\ x>1; f(x)=−1,if \\ x<−1; f(x)=x,otherwise$\n",
    "\n",
    "## nn.Sigmoid\n",
    "\n",
    "## nn.Tanh\n",
    "$Tanh(x) = \\frac{e^{x}-e^{-x} }{e^{x} + e^{-x}}$\n",
    "## nn.LogSigmoid\n",
    "$LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))$\n",
    "\n",
    "## nn.Softplus\n",
    "nn.Softplus(beta=1, threshold=20)   \n",
    "$Softplus(x) = \\frac{1}{beta}*log(1+e^{(beta*x_i)})$ \n",
    "## nn.Softshrink  \n",
    "nn.Softshrink(lambd=0.5)   \n",
    "$f(x)=x−lambda,if\\ x>lambda; f(x)=x+lambda,if\\ x<−lambda; f(x)=0,otherwise$\n",
    "## nn.Softsign\n",
    "$f(x) = x / (1 + |x|)$\n",
    "## nn.Softshrink\n",
    "nn.Softshrink(lambd=0.5)\n",
    "\n",
    "$Tanhshrink(x)=x−Tanh(x)$\n",
    "## nn.Softmin\n",
    "## nn.Softmax\n",
    "## nn.LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm1d\n",
    "## BatchNorm2d\n",
    "## BatchNorm3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.BatchNorm1d(num_features, eps=1e-05, \n",
    "                             momentum=0.1, affine=True)**\n",
    "* 参数说明：  \n",
    "num_features： 来自期望输入的特征数，该期望输入的大小为'batch_size x num_features [x width]'   \n",
    "eps： 为保证数值稳定性（分母不能趋近或取0给分母加上的值。默认为1e-5。   \n",
    "momentum： 动态均值和动态方差所使用的动量。默认为0.1。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupNorm\n",
    "**torch.nn.GroupNorm(num_groups: int, num_channels: int, eps: float = 1e-05, affine: bool = True)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "**torch.nn.LayerNorm(normalized_shape: Union[int, List[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.RNN( args, * kwargs)** \n",
    " $$ h_t=tanh(w_{ih} x_t+b_{ih}+w_{hh} h_{t-1}+b_{hh}) $$\n",
    "\n",
    "* 参数说明:  \n",
    "\n",
    "input_size – 输入x的特征数量。  \n",
    "hidden_size – 隐层的特征数量。    \n",
    "num_layers – RNN的层数。    \n",
    "nonlinearity – 指定非线性函数使用tanh还是relu。默认是tanh。    \n",
    "bias – 如果是False，那么RNN层就不会使用偏置权重 $b_ih$和$b_hh$,默认是True   \n",
    "batch_first – 如果True的话，那么输入Tensor的shape应该是[batch_size, time_step, feature],输出也是这样。  \n",
    "dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个dropout层。   \n",
    "bidirectional – 如果True，将会变成一个双向RNN，默认为False。    \n",
    "* shape :   \n",
    "输入 ： (input, h_0)    \n",
    "      input : (seq_len, batch, input_size)   \n",
    "      h_0 : (num_layers * num_directions, batch, hidden_size)    \n",
    "输出 : (output, h_n)   \n",
    "      output ：(seq_len, batch, hidden_size * num_directions)   \n",
    "      h_n ：(num_layers * num_directions, batch, hidden_size)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.LSTM**   \n",
    "    $$ i_t = sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) $$\n",
    "    $$ f_t = sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) $$\n",
    "    $$ o_t = sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho}) $$ \n",
    "    $$ g_t = tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg}) $$ \n",
    "    $$ c_t = f_tc_{t-1}+i_tg_t $$\n",
    "    $$ h_t = o_t*tanh(c_t)  $$\n",
    "    \n",
    "$h_t$是时刻$t$的隐状态,$c_t$是时刻$t$的细胞状态，$x_t$是上一层的在时刻$t$的隐状态或者是第一层在时刻$t$的输入。$i_t, f_t, g_t, o_t$ 分别代表 输入门，遗忘门，细胞和输出门\n",
    "\n",
    "* 参数说明:   \n",
    "input_size – 输入的特征维度   \n",
    "hidden_size – 隐状态的特征维度    \n",
    "num_layers – 层数（和时序展开要区分开）    \n",
    "bias – 如果为False，那么LSTM将不会使用$b_{ih},b_{hh}$，默认为True。    \n",
    "batch_first – 如果为True，那么输入和输出Tensor的形状为(batch, seq, feature)     \n",
    "dropout – 如果非零的话，将会在RNN的输出上加个dropout，最后一层除外。    \n",
    "bidirectional – 如果为True，将会变成一个双向RNN，默认为False。   \n",
    "* shape :\n",
    " 输入：  input, (h_0, c_0)\n",
    "      input ： (seq_len, batch, input_size)\n",
    "      h_0 ：(num_layers * num_directions, batch, hidden_size)\n",
    "      c_0 ：(num_layers * num_directions, batch, hidden_size)\n",
    " 输出: output, (h_n, c_n)   \n",
    "      output: (seq_len, batch, hidden_size * num_directions)\n",
    "      h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "      c_n: (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(10,20,2)\n",
    "input = torch.randn(5,3,10)\n",
    "h0=torch.randn(2,3,20)\n",
    "c0=torch.randn(2,3,20)\n",
    "out,hn = lstm(input,(h0,c0))\n",
    "print(out.size(),hn[0].size(),hn[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class torch.nn.GRU**\n",
    "$$  r_t=sigmoid(W_{ir}x_t+b_{ir}+W_{hr}h_{(t-1)}+b_{hr})$$ \n",
    "$$  i_t=sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{(t-1)}+b_{hi})$$ \n",
    "$$  n_t=tanh(W_{in}x_t+b_{in}+rt(W_{hn}h_{(t-1)}+b_{hn}))$$ \n",
    "$$  h_t=(1-i_t) nt+i_t*h(t-1) $$ \n",
    "$h_t$是是时间$t$的上的隐状态，$x_t$是前一层$t$时刻的隐状态或者是第一层的$t$时刻的输入，$r_t, i_t, n_t$分别是重置门，输入门和新门。\n",
    "* 参数说明：    \n",
    "input_size – 期望的输入$x$的特征值的维度      \n",
    "hidden_size – 隐状态的维度  \n",
    "num_layers – RNN的层数。    \n",
    "bias – 如果为False，那么RNN层将不会使用bias，默认为True。    \n",
    "batch_first – 如果为True的话，那么输入和输出的tensor的形状是(batch, seq, feature)。 -    \n",
    "dropout – 如果非零的话，将会在RNN的输出上加个dropout，最后一层除外。 -    \n",
    "bidirectional – 如果为True，将会变成一个双向RNN，默认为False。   \n",
    "\n",
    "  输入： input, h_0      \n",
    "      input (seq_len, batch, input_size)    \n",
    "      h_0 (num_layers * num_directions, batch, hidden_size)   \n",
    "  输出： output, h_n   \n",
    "      output (seq_len, batch, hidden_size * num_directions)   \n",
    "      h_n (num_layers * num_directions, batch, hidden_size)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNCell\n",
    "RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')\n",
    "## LSTMCell\n",
    "LSTMCell(input_size, hidden_size, bias=True)\n",
    "## GRUCell\n",
    "GRUCell(input_size, hidden_size, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformer\n",
    "**torch.nn.Transformer(d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = 'relu', custom_encoder: Optional[Any]=None, custom_decoder:Optional[Any] = None)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoder\n",
    "**torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerDecoder\n",
    "**torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoderLayer\n",
    "**torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerDecoderLayer\n",
    "**torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "**class torch.nn.Linear(in_features, out_features, bias=True)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "**class torch.nn.Dropout(p=0.5, inplace=False)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "**class torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,\n",
    "                           max_norm=None,norm_type=2,scale_grad_by_freq=False,sparse=False)**\n",
    "* 参数说明：   \n",
    "num_embeddings (int) - 嵌入字典的大小   \n",
    "embedding_dim (int) - 每个嵌入向量的大小   \n",
    "padding_idx (int, optional) - 如果提供的话，输出遇到此下标时用零填充   \n",
    "max_norm (float, optional) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值   \n",
    "norm_type (float, optional) - 对于max_norm选项计算p范数时的p   \n",
    "scale_grad_by_freq (boolean, optional) - 如果提供的话，会根据字典中单词频率缩放梯度                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pairwiseDistance\n",
    "**class torch.nn.PairwiseDistance(p=2, eps=1e-06)**    \n",
    "按批计算向量v1, v2之间的距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CosineSimilarity\n",
    "**torch.nn.CosineSimilarity(dim: int = 1, eps: float = 1e-08)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1Loss\n",
    "**class torch.nn.L1Loss(size_average=True)**   \n",
    "x,y之间差的绝对值的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss\n",
    "**class torch.nn.MSELoss(size_average=True)**   \n",
    "均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCELoss\n",
    "**class torch.nn.BCELoss(weight=None, size_average=True)**    \n",
    "二进制交叉熵   \n",
    " $$ loss(o,t)=-\\frac{1}{n}\\sum_i(t[i] log(o[i])+(1-t[i]) log(1-o[i])) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "**class torch.nn.CrossEntropyLoss(weight=None, size_average=True)**    \n",
    "交叉熵\n",
    "\n",
    "* nn.CrossEntropyLoss()为交叉熵损失函数，用于解决多分类问题，也可用于解决二分类问题。   \n",
    "* BCELoss是Binary CrossEntropyLoss的缩写，nn.BCELoss()为二元交叉熵损失函数，只能解决二分类问题。   \n",
    "* 在使用nn.CrossEntropyLoss()其内部会自动加上Sofrmax层。\n",
    "* 在使用nn.BCELoss()作为损失函数时，需要在该层前面加上Sigmoid函数，一般使用nn.Sigmoid()即可，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss\n",
    "**class torch.nn.NLLLoss(weight=None, size_average=True)**  \n",
    "CrossEntropyLoss()=log_softmax() + NLLLoss()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss2d\n",
    "**class torch.nn.NLLLoss2d(weight=None, size_average=True)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLDivLoss\n",
    "**class torch.nn.KLDivLoss(weight=None, size_average=True)**   \n",
    "KL散度     \n",
    "$loss(x,target)=1n∑i(targeti∗(log(targeti)−xi))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarginRankingLoss\n",
    "**class torch.nn.MarginRankingLoss(margin=0, size_average=True)**\n",
    "$$loss(x,y)=max(0,−y∗(x1−x2)+margin)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpsamplingNearest2d\n",
    "**class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)**   \n",
    "对于多channel 输入 进行 2-D 最近邻上采样    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpsamplingBilinear2d\n",
    "**class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)**    \n",
    "对于多channel 输入 进行 2-D bilinear 上采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.clip_grad_norm \n",
    "**torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)**\n",
    "\n",
    "如果梯度超过阈值，那么就截断，将梯度变为阈值\n",
    "\n",
    "g = 阀值/|g| * g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.rnn.PackedSequence  \n",
    "* 不需要单独创建 通过utils.rnn.pack_padded_sequence 或utils.rnn.pad_packed_sequence 创建   \n",
    "**torch.nn.utils.rnn.PackedSequence(_cls, data, batch_sizes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.rnn.pack_padded_sequence \n",
    "* 将一个填充过的变长序列压紧   \n",
    "**orch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)**\n",
    "* 参数说明：   \n",
    "enforce_sorted: 强制排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[1.],\n",
      "        [4.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [5.],\n",
      "        [3.]]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))\n",
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [5.],\n",
      "        [3.]]) tensor([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size =3\n",
    "max_length= 3\n",
    "hidden_size = 3\n",
    "n_layers = 1\n",
    "input = torch.autograd.Variable(torch.Tensor([[1,0,0],[1,2,3],[4,5,0]]).view(3,3,1))\n",
    "seq_len =[1,3,2]\n",
    "packed = nn.utils.rnn.pack_padded_sequence(input,seq_len,batch_first=True,enforce_sorted=False)\n",
    "print(packed)\n",
    "print(packed.data,packed.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.utils.rnn.pad_packed_sequence\n",
    "* 填充打包的可变长度序列批次   \n",
    "**torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1.],\n",
      "         [1.],\n",
      "         [4.]],\n",
      "\n",
      "        [[0.],\n",
      "         [2.],\n",
      "         [5.]],\n",
      "\n",
      "        [[0.],\n",
      "         [3.],\n",
      "         [0.]]]), tensor([1, 3, 2]))\n"
     ]
    }
   ],
   "source": [
    "padded = nn.utils.rnn.pad_packed_sequence(packed)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Flatten(start_dim: int = 1, end_dim: int = -1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.082px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
